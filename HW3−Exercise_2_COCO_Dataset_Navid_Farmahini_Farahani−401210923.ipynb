{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#In the name of God\n",
        "\n",
        "#Libraries\n",
        "\n",
        "\n",
        "student_id = 401210923\n",
        "student_name = \"Navid Farahani\"\n",
        "\n",
        "print(\"your student id:\", student_id)\n",
        "print(\"your name:\", student_name)\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "import cv2\n",
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import time\n",
        "\n",
        "from torchvision.datasets import CocoDetection\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "\n",
        "print(\"For better understanding CNNs, I watched this video from Youtube: https://www.youtube.com/watch?v=d9QHNkD_Pos\")\n",
        "print(\"also, I read the below article for better understanding deformable CNNs:\\n [1] J. Dai et al., Deformable convolutional networks, in Proceedings of the IEEE international conference on computer vision, 2017, pp. 764-773. \")"
      ],
      "metadata": {
        "id": "lxvr98XggJEZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d7d9f72-a334-4c79-ce9e-db373c4dadc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "your student id: 401210923\n",
            "your name: Navid Farahani\n",
            "For better understanding CNNs, I watched this video from Youtube: https://www.youtube.com/watch?v=d9QHNkD_Pos\n",
            "also, I read the below article for better understanding deformable CNNs:\n",
            " [1] J. Dai et al., Deformable convolutional networks, in Proceedings of the IEEE international conference on computer vision, 2017, pp. 764-773. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "g0FtdQFKY8Ev",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc4a47e-3c14-4eda-bec3-8f4991ea1c6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using COCO Dataset\n",
        "# We first need to define a class for concatenating the labels and the images.\n",
        "# I considered val2017 images that are 5000 and have (80 Correct!!!) labels\n",
        "# because the other labels where empty,\n",
        "# I save the empty labels as bad_numbers, for example index=250 was empty\n",
        "# then I used np.unique(labels) and could understand that we have 80 correct labels\n",
        "# so, in the Neural Network, in the last layer we should have 80 neurons because we have\n",
        "# 80 classes.\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,img_dir,label_dir):\n",
        "\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir=label_dir\n",
        "        self.img_files = os.listdir(self.img_dir)\n",
        "        self.num_img = int(len(self.img_files))\n",
        "        self.coco_dataset = CocoDetection(root=self.img_dir, annFile=self.label_dir)\n",
        "\n",
        "    def __getitem__(self,index1):\n",
        "        bad_numbers=[250,370,384,431,516,566,838,862,1015,1066,1501,1524,\n",
        "         1737,1749,1961,1988,2098,2253,2265,2315,2321,2336,2397,2663,2687,\n",
        "         2713,2769,2856,2970,3216,3242,3314,3466,3492,3851,3956,3986,4127,4133,\n",
        "         4169,4344,4439,4557,4623,4659,4731,4779,4809]\n",
        "        if index1 not in bad_numbers:\n",
        "            #print(index1)\n",
        "            # in the COCO dataset, [1][0]['category_id'] represents the label\n",
        "            label=self.coco_dataset[int(index1)][1][0]['category_id']\n",
        "\n",
        "            img_file = os.path.join(self.img_dir, self.img_files[index1])\n",
        "            imggg=cv2.imread(img_file)\n",
        "            the_imggg=cv2.resize(imggg,(32,32))\n",
        "        else:\n",
        "            # For bad numbers, we can consider a non bad number ( for example 0)\n",
        "            # for the dataset.\n",
        "            label=self.coco_dataset[int(0)][1][0]['category_id']\n",
        "\n",
        "            img_file = os.path.join(self.img_dir, self.img_files[0])\n",
        "            imggg=cv2.imread(img_file)\n",
        "            the_imggg=cv2.resize(imggg,(32,32))\n",
        "\n",
        "\n",
        "        return (the_imggg,label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 5000\n",
        "\n",
        "# Note that the number of the images is 5000, so for __len__ we considered 5000.\n",
        "\n",
        "\n",
        "# Using COCO dataset from my Google Drive\n",
        "\n",
        "dataset_train = CustomDataset('drive/MyDrive/val2017','drive/MyDrive/annotations/instances_val2017.json')\n",
        "\n",
        "\n",
        "num_images=len(dataset_train)\n",
        "num_train=int(num_images*0.7)\n",
        "num_validation=int(num_images*0.15)\n",
        "num_test=int(num_images*0.15)\n",
        "# Create a DataLoader for the custom dataset\n",
        "train_dataset,validation_dataset,test_dataset=random_split(dataset_train,[num_train,num_validation,num_test])\n",
        "\n",
        "\n",
        "batch_size=64\n",
        "train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True)\n",
        "val_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "def to_device(data, device):\n",
        "    # for every batch, we pass the data to the current device.\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl=dl\n",
        "        self.device=device\n",
        "\n",
        "    def __iter__(self):\n",
        "        #For every batch, we pass it to the current device.\n",
        "        for b in self.dl:\n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dl)\n",
        "\n",
        "# Selecting the current device\n",
        "if torch.cuda.is_available():\n",
        "      device=torch.device('cuda')\n",
        "else:\n",
        "      device=torch.device('cpu')\n",
        "\n",
        "# We move train, validation and test loaders to current device for every batch by using this code\n",
        "train_dl = DeviceDataLoader(train_loader, device)\n",
        "val_dl = DeviceDataLoader(val_loader, device)\n",
        "test_dl = DeviceDataLoader(test_loader, device)\n"
      ],
      "metadata": {
        "id": "6S3GnLCiX-Rg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a441f947-60bb-488c-ec7a-ca2864a20cb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=2.02s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############# Traditional CNN\n",
        "\n",
        "# Define the Traditional CNN model\n",
        "class TraditionalConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TraditionalConvolutionalNetwork, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)# MNIST has just one channel, so the first conv, consider only one layer.\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)# For the last layer which is a FC layer, we just have 10 neurons, because in MNIST we have 10 classes.\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.pool(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.pool(x)\n",
        "        x=x.view(-1, 64 * 7 * 7)  # For Fully Connected Layer, we need to flaten the data.\n",
        "        x=self.fc1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "Pc60OWYsYASP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "متاسفانه بنده در این ایمیل، دیتاست را بر روی گوگل درایو کرده بودم اما جی پی یوی آن تمام شد و شبیه سازی نهایی انجام نشد! اما دیتاست لود شده است؛ ضمناً این تمرین با دیتاست ام نیست شبیه سازی شد."
      ],
      "metadata": {
        "id": "QC42wH8-_pOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=TraditionalConvolutionalNetwork()\n",
        "\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "accuracy=np.zeros(num_epochs)\n",
        "losses=np.zeros(num_epochs)\n",
        "\n",
        "tic=time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images,labels in train_dl:\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        acc=0\n",
        "        all_samples=0\n",
        "        for images,labels in val_dl:\n",
        "            images=images.to(device)\n",
        "            labels=labels.to(device)\n",
        "            outputs=model(images)\n",
        "            _,predicted=torch.max(outputs,1)\n",
        "            all_samples=all_samples+labels.size(0)\n",
        "            acc=acc+(predicted==labels).sum().item()\n",
        "\n",
        "        accuracy[epoch] = acc / all_samples\n",
        "        losses[epoch]=loss\n",
        "        print(\"Epoch \",epoch,\"\\nLoss:\",round(losses[epoch],4),\"Accuracy:\",round(accuracy[epoch],4))\n",
        "\n",
        "toc=time.time()\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    acc=0\n",
        "    all_samples=0\n",
        "    for images,labels in val_dl:\n",
        "          images=images.to(device)\n",
        "          labels=labels.to(device)\n",
        "          outputs=model(images)\n",
        "          _,predicted=torch.max(outputs,1)\n",
        "          all_samples=all_samples+labels.size(0)\n",
        "          acc=acc+(predicted==labels).sum().item()\n",
        "\n",
        "    accuracy_for_test = acc / all_samples\n",
        "    print(\"Accuracy for Test:\",round(accuracy_for_test,4))\n",
        "\n"
      ],
      "metadata": {
        "id": "Ju-x_JNuYC-4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "45f044e1-f23e-47e5-a1d1-c76b416bc1e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Internal Python error in the inspect module.\n",
            "Below is the traceback from this internal error.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-5-33e12c848be2>\", line 18, in <cell line: 16>\n",
            "    for images,labels in train_dl:\n",
            "  File \"<ipython-input-3-58ae9e9afadd>\", line 82, in __iter__\n",
            "    for b in self.dl:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 630, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 674, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 49, in fetch\n",
            "    data = self.dataset.__getitems__(possibly_batched_index)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 364, in __getitems__\n",
            "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\", line 364, in <listcomp>\n",
            "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
            "  File \"<ipython-input-3-58ae9e9afadd>\", line 26, in __getitem__\n",
            "    label=self.coco_dataset[int(index1)][1][0]['category_id']\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/coco.py\", line 48, in __getitem__\n",
            "    image = self._load_image(id)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torchvision/datasets/coco.py\", line 41, in _load_image\n",
            "    return Image.open(os.path.join(self.root, path)).convert(\"RGB\")\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/PIL/Image.py\", line 3236, in open\n",
            "    prefix = fp.read(16)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n",
            "    stb = value._render_traceback_()\n",
            "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n",
            "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n",
            "    return f(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n",
            "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1662, in getinnerframes\n",
            "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 1620, in getframeinfo\n",
            "    filename = getsourcefile(frame) or getfile(frame)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 829, in getsourcefile\n",
            "    module = getmodule(object, filename)\n",
            "  File \"/usr/lib/python3.10/inspect.py\", line 878, in getmodule\n",
            "    os.path.realpath(f)] = module.__name__\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 396, in realpath\n",
            "    path, ok = _joinrealpath(filename[:0], filename, strict, {})\n",
            "  File \"/usr/lib/python3.10/posixpath.py\", line 431, in _joinrealpath\n",
            "    st = os.lstat(newpath)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-33e12c848be2>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-58ae9e9afadd>\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;31m#For every batch, we pass it to the current device.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mto_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__getitems__\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-58ae9e9afadd>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index1)\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# in the COCO dataset, [1][0]['category_id'] represents the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/coco.py\u001b[0m in \u001b[0;36m_load_image\u001b[0;34m(self, id)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3236\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Traditional_required_time=toc-tic\n",
        "Accuracy_Traditional=accuracy\n",
        "Losses_Traditional=losses\n",
        "Accuracy_Traditional_test=accuracy_for_test\n",
        "\n",
        "\n",
        "print(\"Required time for simulation for Traditional CNN:\"+str(Traditional_required_time)+\"s\")\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(accuracy)\n",
        "plt.title(\"Accuracy Traditional\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(losses)\n",
        "plt.title(\"Losses Traditional\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")"
      ],
      "metadata": {
        "id": "4m-QZxYDYEoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Deformable CNN\n",
        "\n",
        "# As I mentioned in the report of theoric questions, we need to consider offsets as some parameters and multiple these to\n",
        "# the position of the samples of the grid and displace them.\n",
        "# These offsets are learnable and during the training, we train it in an end-to-end form.\n",
        "class DeformableConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super(DeformableConv2d, self).__init__()\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.padding = padding\n",
        "\n",
        "\n",
        "        # We can consider 9 offsets for all samples of the kernels(kernels are 3 by 3) for all input channels. If the grid is h * w, we have h*w grid samples.\n",
        "        # So we can update these parameters if we consider these samples as a conv layer with an arbitrary stride and padding and kernel size.\n",
        "        # In fact we train the position of the offsets by help of a specific nn.Conv2d and we use for all possible kernels due to striding and paddings and finally\n",
        "        # we multiple this grids to the Traditional Kernel.\n",
        "\n",
        "        # Note that we have 2 offests for x and y directs, so the output channels must be 2*kernel_size*kernel_size.\n",
        "\n",
        "        self.conv_offset = nn.Conv2d(in_channels, 2 * kernel_size * kernel_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "\n",
        "        # We define conv_weight for the image for extracting features. But in the forward method, we multiple the offsets to this weights\n",
        "        # And we finally use this weights instead of the module nn.Conv2d for extracting features in a layer.\n",
        "\n",
        "        self.conv_weight = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "        # We use this function for passing the scattered grid and find the best position by extracting features by using a nn.Conv2d as\n",
        "        self.Conv_For_Grid= nn.Conv2d(kernel_size * kernel_size, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Calculating offsets\n",
        "        offsets = self.conv_offset(x)\n",
        "\n",
        "        # Generate grid for sampling\n",
        "        # For every batch and channel, we consider grid_y and grid_x to construct the structured grid(they don't have irregular shapes yet)\n",
        "\n",
        "        grid_y=offsets[:,:self.kernel_size*self.kernel_size, :, :]\n",
        "        grid_x=offsets[:,self.kernel_size*self.kernel_size:, :, :]\n",
        "        #Size of offests: (batchs,kernel**2,h,w)\n",
        "\n",
        "        # Normalize grid to range [-1, 1]\n",
        "        # According to the referenced article, we can define the R region in the interval of [-1,1], so if we normalize it we can construct this R\n",
        "        grid_y = (2 * grid_y / (x.size(-2) - 1)) - 1\n",
        "        grid_x = (2 * grid_x / (x.size(-1) - 1)) - 1\n",
        "\n",
        "        sampled_features=(grid_x+grid_y)/2\n",
        "\n",
        "        # After constructing the grid, we pass it from a network and it will be trained, so we can multiple this offsets to the\n",
        "        # traditional kernel which is called \"weights\".\n",
        "        sampled_features=self.Conv_For_Grid(sampled_features)\n",
        "        weights = self.conv_weight(x)\n",
        "        output = sampled_features * weights\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ukwOtl5JYGoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the CNN model\n",
        "class DeformableConvolutionalNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DeformableConvolutionalNetwork, self).__init__()\n",
        "\n",
        "        self.conv1=DeformableConv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.relu=nn.ReLU()\n",
        "        self.pool=nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2=DeformableConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1=nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2=nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x=self.conv1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.pool(x)\n",
        "        x=self.conv2(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.pool(x)\n",
        "        x=x.view(-1, 64 * 7 * 7)\n",
        "        x=self.fc1(x)\n",
        "        x=self.relu(x)\n",
        "        x=self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = DeformableConvolutionalNetwork()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "accuracy=np.zeros(num_epochs)\n",
        "losses=np.zeros(num_epochs)\n",
        "\n",
        "tic=time.time()\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for images,labels in train_dl:\n",
        "        images=images.to(device)\n",
        "        labels=labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs=model(images)\n",
        "        loss=criterion(outputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        acc=0\n",
        "        all_samples=0\n",
        "        for images,labels in val_dl:\n",
        "            images=images.to(device)\n",
        "            labels=labels.to(device)\n",
        "            outputs=model(images)\n",
        "            _,predicted=torch.max(outputs,1)\n",
        "            all_samples=all_samples+labels.size(0)\n",
        "            acc=acc+(predicted==labels).sum().item()\n",
        "\n",
        "        accuracy[epoch] = acc / all_samples\n",
        "        losses[epoch]=loss\n",
        "        print(\"Epoch \",epoch,\"\\nLoss:\",round(losses[epoch],4),\"Accuracy:\",round(accuracy[epoch],4))\n",
        "\n",
        "toc=time.time()\n",
        "\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "    acc=0\n",
        "    all_samples=0\n",
        "    for images,labels in val_dl:\n",
        "          images=images.to(device)\n",
        "          labels=labels.to(device)\n",
        "          outputs=model(images)\n",
        "          _,predicted=torch.max(outputs,1)\n",
        "          all_samples=all_samples+labels.size(0)\n",
        "          acc=acc+(predicted==labels).sum().item()\n",
        "\n",
        "    accuracy_for_test = acc / all_samples\n",
        "    print(\"Accuracy for Test:\",round(accuracy_for_test,4))\n"
      ],
      "metadata": {
        "id": "9MLuEek9YH7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Deformable_required_time=toc-tic\n",
        "Accuracy_Deformable=accuracy\n",
        "Losses_Deformable=losses\n",
        "Accuracy_Deformable_test=accuracy_for_test\n",
        "\n",
        "print(\"Required time for simulation for Deformable CNN:\"+str(Deformable_required_time)+\"s\")\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(accuracy)\n",
        "plt.title(\"Accuracy Deformable\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(losses)\n",
        "plt.title(\"Losses Deformable \")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epochs\")\n"
      ],
      "metadata": {
        "id": "AD3kN7RVYLS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############# Comparing Two Methods\n",
        "print(\"Accuracy for Deformable is  \"+str(100*float(Accuracy_Deformable[-1]-Accuracy_Traditional[-1]))+\"%  more than traditional CNN\")\n",
        "print(\"Required time for Deformable is  \"+str(round(float(Deformable_required_time-Traditional_required_time),4))+\"s  more than traditional CNN\")\n"
      ],
      "metadata": {
        "id": "p1n_qx7XYNzI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}